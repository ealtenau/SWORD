"""Output stage for v17c pipeline â€” save to DuckDB + SWOT slopes."""

import glob as glob_mod
import json
import os
from typing import Dict, Optional

import duckdb
import numpy as np
import pandas as pd

from ._logging import log


def save_to_duckdb(
    conn: duckdb.DuckDBPyConnection,
    region: str,
    hydro_dist: Dict[int, Dict],
    hw_out: Dict[int, Dict],
    is_mainstem: Dict[int, bool],
    main_neighbors: Optional[Dict[int, Dict]] = None,
    path_vars: Optional[Dict[int, Dict]] = None,
) -> int:
    """
    Save computed v17c attributes to DuckDB reaches table.

    Returns:
        Number of reaches updated
    """
    log(f"Saving v17c attributes to DuckDB for {region}...")

    # Build update dataframe
    rows = []
    mn = main_neighbors or {}
    pv = path_vars or {}
    for reach_id in hydro_dist.keys():
        hd = hydro_dist.get(reach_id, {})
        ho = hw_out.get(reach_id, {})
        ms = is_mainstem.get(reach_id, False)
        nb = mn.get(reach_id, {})
        pvar = pv.get(reach_id, {})

        row = {
            "reach_id": reach_id,
            "hydro_dist_out": hd.get("hydro_dist_out"),
            "hydro_dist_hw": hd.get("hydro_dist_hw"),
            "best_headwater": ho.get("best_headwater"),
            "best_outlet": ho.get("best_outlet"),
            "pathlen_hw": ho.get("pathlen_hw"),
            "pathlen_out": ho.get("pathlen_out"),
            "is_mainstem_edge": ms,
            "rch_id_up_main": nb.get("rch_id_up_main"),
            "rch_id_dn_main": nb.get("rch_id_dn_main"),
        }
        if pvar:
            row["path_freq"] = pvar.get("path_freq")
            row["stream_order"] = pvar.get("stream_order")
            row["path_segs"] = pvar.get("path_segs")
            row["path_order"] = pvar.get("path_order")
        rows.append(row)

    if not rows:
        log("No rows to update")
        return 0

    update_df = pd.DataFrame(rows)

    # Handle infinity values - convert to NULL
    update_df = update_df.replace([np.inf, -np.inf], np.nan)

    # Register DataFrame and update
    conn.register("v17c_updates", update_df)

    # Load spatial extension (needed for RTREE index compatibility)
    try:
        conn.execute("INSTALL spatial; LOAD spatial;")
    except Exception:
        pass  # Extension may already be loaded or not needed

    # Build SET clause - always include base v17c columns
    set_clauses = [
        "hydro_dist_out = u.hydro_dist_out",
        "hydro_dist_hw = u.hydro_dist_hw",
        "best_headwater = u.best_headwater",
        "best_outlet = u.best_outlet",
        "pathlen_hw = u.pathlen_hw",
        "pathlen_out = u.pathlen_out",
        "is_mainstem_edge = u.is_mainstem_edge",
        "rch_id_up_main = u.rch_id_up_main",
        "rch_id_dn_main = u.rch_id_dn_main",
    ]
    if path_vars:
        set_clauses.extend(
            [
                "path_freq = u.path_freq",
                "stream_order = u.stream_order",
                "path_segs = u.path_segs",
                "path_order = u.path_order",
            ]
        )

    # Update reaches table
    conn.execute(f"""
        UPDATE reaches SET
            {", ".join(set_clauses)}
        FROM v17c_updates u
        WHERE reaches.reach_id = u.reach_id
        AND reaches.region = '{region.upper()}'
    """)

    conn.unregister("v17c_updates")

    log(f"Updated {len(rows):,} reaches")
    return len(rows)


def save_sections_to_duckdb(
    conn: duckdb.DuckDBPyConnection,
    region: str,
    sections_df: pd.DataFrame,
    validation_df: pd.DataFrame,
) -> None:
    """Save section data and validation results to DuckDB tables."""
    log(f"Saving sections to DuckDB for {region}...")

    if sections_df.empty:
        log("No sections to save")
        return

    # Prepare sections for insert
    sections_insert = sections_df.copy()
    sections_insert["region"] = region.upper()
    # Convert reach_ids list to JSON string
    sections_insert["reach_ids"] = sections_insert["reach_ids"].apply(json.dumps)

    conn.register("sections_insert", sections_insert)
    conn.execute("""
        INSERT OR REPLACE INTO v17c_sections
        SELECT
            section_id,
            region,
            upstream_junction,
            downstream_junction,
            reach_ids,
            distance,
            n_reaches
        FROM sections_insert
    """)
    conn.unregister("sections_insert")
    log(f"Saved {len(sections_insert):,} sections")

    # Save validation results if any
    if not validation_df.empty:
        validation_insert = validation_df.copy()
        validation_insert["region"] = region.upper()

        conn.register("validation_insert", validation_insert)
        conn.execute("""
            INSERT OR REPLACE INTO v17c_section_slope_validation
            SELECT
                section_id,
                region,
                slope_from_upstream,
                slope_from_downstream,
                direction_valid,
                likely_cause
            FROM validation_insert
        """)
        conn.unregister("validation_insert")
        log(f"Saved {len(validation_insert):,} validation records")


def apply_swot_slopes(
    conn: duckdb.DuckDBPyConnection,
    region: str,
    swot_path: str,
) -> int:
    """
    Apply SWOT-derived slopes to reaches.

    This function:
    1. Loads SWOT node data from parquet files
    2. Computes section-level slopes with MAD outlier filtering
    3. Updates reaches with swot_slope, swot_slope_se, swot_slope_confidence

    Parameters
    ----------
    conn : duckdb.DuckDBPyConnection
        Database connection
    region : str
        Region code
    swot_path : str
        Path to SWOT parquet files directory

    Returns
    -------
    int
        Number of reaches updated
    """
    log(f"Applying SWOT slopes for {region}...")

    # Check if SWOT data exists
    if not os.path.isdir(swot_path):
        log(f"SWOT data directory not found: {swot_path}")
        return 0

    parquet_files = [
        f
        for f in glob_mod.glob(os.path.join(swot_path, "*.parquet"))
        if not os.path.basename(f).startswith("._")
    ]

    if not parquet_files:
        log(f"No parquet files found in {swot_path}")
        return 0

    log(f"Found {len(parquet_files)} SWOT parquet files")

    # Get node_ids for this region
    nodes_df = conn.execute(
        """
        SELECT node_id FROM nodes WHERE region = ?
    """,
        [region.upper()],
    ).fetchdf()

    if nodes_df.empty:
        log(f"No nodes found for region {region}")
        return 0

    node_ids = nodes_df["node_id"].tolist()
    log(f"Region {region} has {len(node_ids):,} nodes")

    # For now, we'll use the pre-computed slopes if they exist
    # in the SWOT pipeline output
    swot_slopes_file = os.path.join(
        os.path.dirname(swot_path).replace("/node", ""),
        f"output/{region.lower()}/{region.lower()}_swot_slopes.csv",
    )

    if os.path.exists(swot_slopes_file):
        log(f"Loading pre-computed SWOT slopes from {swot_slopes_file}")
        slopes_df = pd.read_csv(swot_slopes_file)

        # Map section slopes to reaches
        # This requires the section-reach mapping which we'd need from the pipeline
        log(f"Loaded {len(slopes_df):,} section slopes")

        # For now, just log that SWOT slopes are available
        # Full integration would require running SWOT_slopes.py functions
        log("SWOT slope integration requires section-reach mapping")
        return 0

    else:
        log(f"SWOT slopes file not found: {swot_slopes_file}")
        log("Run SWOT_slopes.py separately to compute slopes")
        return 0
